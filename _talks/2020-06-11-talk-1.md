---
title: "Numerical Estimation of Information Measures"
collection: talks
type: "Talk"
permalink: /talks/2020-06-11-talk-1
venue: "UC San Francisco, Department of Testing"
date: 2020-06-11
location: "Virtual Conference"
---
Entropy measures the information content of a random quantity. The closely related concept of Mutual Information (MI) between two random variables measures the reduction in uncertainty of one random variable due to information obtained from the other. As an information-theoretic quantity, MI plays an important role in many scientific disciplines. Its applications include decision trees in machine learning, independent component analysis (ICA), gene detection and expression, link prediction, topic discovery, image registration, feature selection and transformations, and channel capacity. There are theoretical and practical properties that make MI superior to other, simpler measures. At the same time, to estimate it from samples becomes a vital and difficult issue. The proposed research aims to develop efficient mutual information estimation systems with a set of estimators that produce accurate results irrespective of sample size, dimensionality, and correlation.
